{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_news_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b1b45979a2284a5f9dfb0c5deff7f6f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_f2ca53a2761f490f8af6e091f6d5b687",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_930246900a094212b40226eca2b8fd89",
              "IPY_MODEL_2fc0c41ce9e64f92ad1222fce80c931c"
            ]
          }
        },
        "f2ca53a2761f490f8af6e091f6d5b687": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "930246900a094212b40226eca2b8fd89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f1698d43b1334fb7930669dae1fcb40f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_da1526e9b5014a41b1e03799e045e4dc"
          }
        },
        "2fc0c41ce9e64f92ad1222fce80c931c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_286be3173a0f41a680f9d1bef8978845",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 996k/996k [00:00&lt;00:00, 1.54MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f327bb1ab7d34851b32e40d056b8e42e"
          }
        },
        "f1698d43b1334fb7930669dae1fcb40f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "da1526e9b5014a41b1e03799e045e4dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "286be3173a0f41a680f9d1bef8978845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f327bb1ab7d34851b32e40d056b8e42e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4518bb6d20e541daa887bed71409a483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_77eeb47af0214919b714618610b5ca82",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_244b95b22a4e455c8a57fbe2fac72cb9",
              "IPY_MODEL_5a94a62da0c84425bbb6d40895f27c6d"
            ]
          }
        },
        "77eeb47af0214919b714618610b5ca82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "244b95b22a4e455c8a57fbe2fac72cb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b4a940374d8e412e8863f611408398ea",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 625,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 625,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bea8470a63214076a835572f0c37a602"
          }
        },
        "5a94a62da0c84425bbb6d40895f27c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c1253b9fc46b4f0791dbfc5ede9a93b9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 625/625 [00:10&lt;00:00, 59.2B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ba0dd743d594330a7d750d38375ae2c"
          }
        },
        "b4a940374d8e412e8863f611408398ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bea8470a63214076a835572f0c37a602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1253b9fc46b4f0791dbfc5ede9a93b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ba0dd743d594330a7d750d38375ae2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "768094f521194f0c984b67731106ff55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e87c5d9386104331b6f5c3bbc10f86aa",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_041e5859104a4c6fb8941a667d38e40f",
              "IPY_MODEL_9b62bcc498b445959ccb35d48f974a0d"
            ]
          }
        },
        "e87c5d9386104331b6f5c3bbc10f86aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "041e5859104a4c6fb8941a667d38e40f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a577060c071342ef8a1079b3381a5b6f",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 714314041,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 714314041,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_572f42454022428c884e3169bb07d858"
          }
        },
        "9b62bcc498b445959ccb35d48f974a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0d0bba95850f4b719b392d4fb9dba380",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 714M/714M [00:09&lt;00:00, 72.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f31c351e7c0541de83bacd4b301a6417"
          }
        },
        "a577060c071342ef8a1079b3381a5b6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "572f42454022428c884e3169bb07d858": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0d0bba95850f4b719b392d4fb9dba380": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f31c351e7c0541de83bacd4b301a6417": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gatherheart/Classification_With_BERT/blob/main/bert_news_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i45d7E0L8bZ_"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# **Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkAHQrj2Vjbl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a771b2-a93b-4c64-c44c-f3fe6470073b"
      },
      "source": [
        "# Hugging Face의 트랜스포머 모델을 설치\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 67.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 64.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=39a740f64ab3a3934907b2f4bfbbd24801d6cc5f25f3e8252cf2ff6fa2c7e33a\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rw4NecgwJylU"
      },
      "source": [
        "BASE_URL = '/content/drive/MyDrive/hu_dataset'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUsHHOgYHB8M",
        "outputId": "90110396-27ef-4412-cd0c-ee5535fe4227"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5eBBp2ZoMz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61632088-4cba-4a4e-bd11-ba7eba9bb00d"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Nov 30 04:58:08 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   33C    P0    23W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75dIz2fNWG8F"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_U3uMySBCIV"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# **Data Load**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImBtAkSyTW1r"
      },
      "source": [
        "df = pd.read_csv(BASE_URL+'/preprocessed_data.csv', index_col=[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz8JzIkdHRSP"
      },
      "source": [
        "labels_csv = pd.read_csv('/content/drive/MyDrive/hu_dataset/labels_dict.csv', index_col=[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3EotSBxqw3x",
        "outputId": "4bb74728-0c1f-495d-f8d8-69ca3a36e140"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['일자', '제목', '통합 분류1', '키워드', '특성추출(가중치순 상위 50개)', '본문', 'label'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RebUkhrBrHEp"
      },
      "source": [
        "df = df[['키워드', '본문', '특성추출(가중치순 상위 50개)', 'label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "R-XOlwMkrKhA",
        "outputId": "d40ee43f-8508-4807-fc8c-f62797d4c5be"
      },
      "source": [
        "df.head(n=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>키워드</th>\n",
              "      <th>본문</th>\n",
              "      <th>특성추출(가중치순 상위 50개)</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>페놀,도시,도시,국회의원,강효상,자유,한국당,비례,대구,달서병,당협,위원장,물클러스...</td>\n",
              "      <td>물클러스터'물기술인증원 잇단 유치 \\n대구, 대한민국 물산업 전진기지로 \\n \\n ...</td>\n",
              "      <td>대구 물산업클러스터 물산업 클러스터 기업인 환경부 물기술인증원 강효상 수돗물 대한민...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>판소리,흥보,정기발표회,명창,정순임,정기,표회,서라벌문화관,경주,판소리,예능,보유자...</td>\n",
              "      <td>판소리 예능보유자인 정순임 명창이 12월 11일(수) 오후 7시 경주 서라벌문화회관...</td>\n",
              "      <td>판소리 정순임 흥보 경상북도 발표회 이수자 경주 남도 제비노정기 정기발표회 문화재 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>정보통신산업진흥원,ICT,콤플렉스,개소,역할,미래,ICT,혁신,인재,선도,기업,성장...</td>\n",
              "      <td>미래를 이끌어갈 ICT 혁신인재와 선도적 기업으로의 성장을 지원하는 SW개발공간 제...</td>\n",
              "      <td>ict콤플렉스 ict 4차 sw 해커톤 100건 베트남 마포구 마포청년혁신타운 서울...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>지원시,대입,정시,지원,사항,전형,마무리,수시모집,합격,학생들,정시모집,집중,이틀,...</td>\n",
              "      <td>이제 모든 수시 전형이 마무리가 되었을 것이다. 수시모집에서 합격하지 못한 학생들은...</td>\n",
              "      <td>서울대 표준점수 백분위 대학별 100점 대구진학지도협의회 상위권 학생들 고려대 서강...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>경상고,개최,유교,책판,순회,전시회,대구,경상고등학교,교장,이철우,교내,강당,유교,...</td>\n",
              "      <td>대구 경상고등학교(교장 이철우)가 최근 교내 강당에서 '찾아가는 유교 책판 순회 전...</td>\n",
              "      <td>전시회 복원본 이철우 무구정광대다라니경 경상고 세계기록유산 책판 도산서당 이원호 유...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>남편,무하마드,단풍,경북,영양,심심산골,그림,운무,염소들,산책,시작,골짜기,염소들,...</td>\n",
              "      <td>단풍이 물들어가는 경북 영양의 심심산골. 그림 같은 운무가 걷히면 염소들의 산책이 ...</td>\n",
              "      <td>현민씨 한국 화선씨 무하마드 파키스탄 마산 현민 박현민 염소들 장모님 닭살부부 아미...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>극한직업,장애인,취준,취업,방송,별일,극한직업,주제,재치,진행,MC,조우종,이동우,...</td>\n",
              "      <td>2일 방송되는 &lt;별일 없이 산다&gt;에서는 ‘극한직업’이라는 주제를 놓고 재치 넘치는 ...</td>\n",
              "      <td>장애인 홍서윤 극한직업 이상미 조우종 이계천 별일 desk 사람들 직업관 김동구 옛...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>대구미래,여성,이야기,영화데이,개최,대구미래,여성,대표,오무선,이야기,영화데이,4일...</td>\n",
              "      <td>(사)대구미래인여성(대표 오무선)은 제6회 '이야기가 있는 영화데이'를 4일(수) ...</td>\n",
              "      <td>긴즈버그 동국대 차세대 대백프라자 하버드대학교 변호인 영화데이 대구미래 수석졸업생 ...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>무관심,남화영,경북,소방본,부장,날씨,동시,화재,발생,증가,추세,소방관서,겨울철,소...</td>\n",
              "      <td>날씨가 추워지면서 자연스레 불을 가까이 하는 시기가 다가와 동시에 화재 발생도 증가...</td>\n",
              "      <td>겨울철 부주의 경북 소방력 경북소방본부 소방관서 일상생활 무각본 남화영 무관심 소방...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>다문화,가족,교류,소통,공간,개소,국가,전통,음식,비치,각국,도서,대구,동구청,다문...</td>\n",
              "      <td>대구 동구청은 다문화가족의 교류소통공간인 '다가온'(ON)이 최근 문을 열었다고 1...</td>\n",
              "      <td>베트남 각국 다문화가족 다문화 교류소통공간 동구 편의시설 체험프로그램 전통음식 7천...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 키워드  ... label\n",
              "0  페놀,도시,도시,국회의원,강효상,자유,한국당,비례,대구,달서병,당협,위원장,물클러스...  ...     0\n",
              "1  판소리,흥보,정기발표회,명창,정순임,정기,표회,서라벌문화관,경주,판소리,예능,보유자...  ...     1\n",
              "2  정보통신산업진흥원,ICT,콤플렉스,개소,역할,미래,ICT,혁신,인재,선도,기업,성장...  ...     2\n",
              "3  지원시,대입,정시,지원,사항,전형,마무리,수시모집,합격,학생들,정시모집,집중,이틀,...  ...     3\n",
              "4  경상고,개최,유교,책판,순회,전시회,대구,경상고등학교,교장,이철우,교내,강당,유교,...  ...     4\n",
              "5  남편,무하마드,단풍,경북,영양,심심산골,그림,운무,염소들,산책,시작,골짜기,염소들,...  ...     5\n",
              "6  극한직업,장애인,취준,취업,방송,별일,극한직업,주제,재치,진행,MC,조우종,이동우,...  ...     6\n",
              "7  대구미래,여성,이야기,영화데이,개최,대구미래,여성,대표,오무선,이야기,영화데이,4일...  ...     7\n",
              "8  무관심,남화영,경북,소방본,부장,날씨,동시,화재,발생,증가,추세,소방관서,겨울철,소...  ...     8\n",
              "9  다문화,가족,교류,소통,공간,개소,국가,전통,음식,비치,각국,도서,대구,동구청,다문...  ...     0\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        },
        "id": "Rozn-qYIhYq_",
        "outputId": "b43f0a25-e27e-402d-e532-ac79ba97dbc8"
      },
      "source": [
        "df[df['label']==10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>키워드</th>\n",
              "      <th>본문</th>\n",
              "      <th>특성추출(가중치순 상위 50개)</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>괴담,연예,재현,프로듀스,사건,시즌,조작,카라,멤버,구하라,사건,한동안,괴담,연예,...</td>\n",
              "      <td>프로듀스 101 전 시즌 조작 사건과 '카라'의 전 멤버 구하라가 갑작스레 숨진 사...</td>\n",
              "      <td>연예계 교통사고 연예인 사건들 강병철 듀스 호기심 연예인들 듯이 김호영</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>2019년,월30일,12월,음력11월,4일,11월,장기,관점,계획,48세,참견,상생...</td>\n",
              "      <td>◇쥐 \\n \\n ▲36세 장기적인 관점에서 계획을 세우는 것이 필요한 시기. ▲48...</td>\n",
              "      <td>흉일 길일 만큼 눈앞 인덕 아랫사람 참이름 결단력 56세 72세 53세 64세 걱정...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>생생정보,무한리필,모둠전,꼬치전,동그랑땡,묵사발,침샘자극,생생정보,모둠전,무한리필,...</td>\n",
              "      <td>‘생생정보’ 7000원 모둠전 무한리필 맛집이 주목을 받았다. \\n\\n28일 오후 ...</td>\n",
              "      <td>모둠전 동그랑땡 생생정보 주인장 kbs2 꼬치전 무한리필 남동구 침샘자극 리얼가 동...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>1500년,비화가야,무덤,63호,창녕,고분,일반,공개,추정,최고,지배자,무덤,도굴,...</td>\n",
              "      <td>창녕 고분군 63호분 일반 공개 5세기 최고 지배자 무덤 추정\\n도굴 없이 온전한 ...</td>\n",
              "      <td>비화가야 63호 39호 연구소 뚜껑돌 창녕 매장주체부 고분군 경북 교동 송현동 주체...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>단국대,동창회,신석주,거보산업,대표,자랑,단국인,선정,단국대,동창회,회장,윤석기,신...</td>\n",
              "      <td>단국대 총동창회(회장 윤석기)는 신석주 (주)거보산업 대표이사를 올해의 자랑스러운 ...</td>\n",
              "      <td>동창회 신석주 더케이호텔서울 거보산업 동문회 시상식 건설환경전문기업 윤석기 단국대 ...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59186</th>\n",
              "      <td>코로나19,취약계층,김장김치,온정,제공,사진,농협,농협,회장,이성희,경제지주,서울광...</td>\n",
              "      <td>사진 농협 제공 \\n농협(회장 이성희) 경제지주가 24일 서울 도봉구 소재 서울광역...</td>\n",
              "      <td>농협 서울 코로나19 5000포기 국민운동 이성희 사회취약계층 취약계층 경제지주 사...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59191</th>\n",
              "      <td>대우조선,선박,제공,사진,대우조선해양,대우조선해양,대표이사,이성근,건조,LNG운반선...</td>\n",
              "      <td>사진 대우조선해양 제공 \\n대우조선해양(대표이사 이성근)은 건조 중인 LNG운반선에...</td>\n",
              "      <td>대우조선해양 이성근 천연가스 lng 대표이사 운반선 옥포조선소 조선업 선적작업 ln...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59596</th>\n",
              "      <td>7시간,사랑,제일교회,명도,집행,중단,10여,부상,장위동,서울,성북구,사랑제일교회,...</td>\n",
              "      <td>서울 성북구 장위동의 사랑제일교회에 대한 법원의 명도집행이 신도들의 저항에 부딪혀 ...</td>\n",
              "      <td>서울 신도 10여 사랑제일교회 명도집행 성북구 제일교회 부상자 소방당국 화염병 장위...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59753</th>\n",
              "      <td>재정난,순복음교회,알짜,여의도,알짜,매물,여의도순복음교회,소유,서울,여의도,금싸라기...</td>\n",
              "      <td>[아시아경제 문제원 기자] 여의도순복음교회 소유 서울 여의도의 8264㎡짜리 금싸라...</td>\n",
              "      <td>여의도 코로나19 여의도순복음교회 관계자 학교용지 신도 서울 부동산 금호리첸시아 순...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59832</th>\n",
              "      <td>양평군,25년,장수음식점,음식점,대상,장수,음식점,선정,양평군,군수,정동균,양평군,...</td>\n",
              "      <td>[헤럴드경제(양평)=박준환 기자]양평군(군수 정동균)이 지난 24일 양평군만이 가지...</td>\n",
              "      <td>양평군 장수음식점 음식점 진지상 콩리 전문가 신내보리밥 보리밥 정동균 만큼 배연정 ...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>271 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     키워드  ... label\n",
              "11     괴담,연예,재현,프로듀스,사건,시즌,조작,카라,멤버,구하라,사건,한동안,괴담,연예,...  ...    10\n",
              "31     2019년,월30일,12월,음력11월,4일,11월,장기,관점,계획,48세,참견,상생...  ...    10\n",
              "76     생생정보,무한리필,모둠전,꼬치전,동그랑땡,묵사발,침샘자극,생생정보,모둠전,무한리필,...  ...    10\n",
              "99     1500년,비화가야,무덤,63호,창녕,고분,일반,공개,추정,최고,지배자,무덤,도굴,...  ...    10\n",
              "181    단국대,동창회,신석주,거보산업,대표,자랑,단국인,선정,단국대,동창회,회장,윤석기,신...  ...    10\n",
              "...                                                  ...  ...   ...\n",
              "59186  코로나19,취약계층,김장김치,온정,제공,사진,농협,농협,회장,이성희,경제지주,서울광...  ...    10\n",
              "59191  대우조선,선박,제공,사진,대우조선해양,대우조선해양,대표이사,이성근,건조,LNG운반선...  ...    10\n",
              "59596  7시간,사랑,제일교회,명도,집행,중단,10여,부상,장위동,서울,성북구,사랑제일교회,...  ...    10\n",
              "59753  재정난,순복음교회,알짜,여의도,알짜,매물,여의도순복음교회,소유,서울,여의도,금싸라기...  ...    10\n",
              "59832  양평군,25년,장수음식점,음식점,대상,장수,음식점,선정,양평군,군수,정동균,양평군,...  ...    10\n",
              "\n",
              "[271 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKceTBZaIDvC",
        "outputId": "af9c9dce-981e-47a5-b2a3-489949068a37"
      },
      "source": [
        "print(set(df['label']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWM6a5ScIPEI"
      },
      "source": [
        "NUM_LABELS = max(set(df['label'])) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFy6QpR_IT1Q",
        "outputId": "f826b219-1802-4c21-ccbd-fee5233b42f0"
      },
      "source": [
        "NUM_LABELS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4796eNIIKJ0"
      },
      "source": [
        "text_to_label = dict(labels_csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUPKFkHeHmOx",
        "outputId": "f315d737-5c02-48a8-edb2-7cbf50165e24"
      },
      "source": [
        "int(text_to_label['IT_과학>IT_과학일반'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jsFUJV-IhrA"
      },
      "source": [
        "label_to_text = {}\n",
        "for label in text_to_label:\n",
        "    label_to_text[int(text_to_label[label])] = label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcEcNAGjHfwg",
        "outputId": "80ba3386-e255-48e1-f7db-fda2f62a46e3"
      },
      "source": [
        "label_to_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '지역>대구',\n",
              " 1: '문화>음악',\n",
              " 2: '경제>취업_창업',\n",
              " 3: '사회>교육_시험',\n",
              " 4: '문화>전시_공연',\n",
              " 5: '문화>방송_연예',\n",
              " 6: '사회>장애인',\n",
              " 7: '사회>여성',\n",
              " 8: '지역>대전',\n",
              " 9: '지역>경남',\n",
              " 10: '문화>문화일반',\n",
              " 11: '문화>학술_문화재',\n",
              " 12: 'IT_과학>콘텐츠',\n",
              " 13: '문화>출판',\n",
              " 14: '문화>요리_여행',\n",
              " 15: '미분류',\n",
              " 16: '지역>충북',\n",
              " 17: '국제>중국',\n",
              " 18: '지역>경북',\n",
              " 19: '사회>미디어',\n",
              " 20: '경제>유통',\n",
              " 21: '경제>부동산',\n",
              " 22: '경제>경제일반',\n",
              " 23: '사회>사건_사고',\n",
              " 24: '사회>노동_복지',\n",
              " 25: '정치>청와대',\n",
              " 26: '국제>일본',\n",
              " 27: '사회>사회일반',\n",
              " 28: '스포츠>월드컵',\n",
              " 29: 'IT_과학>보안',\n",
              " 30: '스포츠>농구_배구',\n",
              " 31: '문화>생활',\n",
              " 32: 'IT_과학>모바일',\n",
              " 33: '정치>북한',\n",
              " 34: '정치>국회_정당',\n",
              " 35: '정치>정치일반',\n",
              " 36: '스포츠>축구>해외축구',\n",
              " 37: '정치>외교',\n",
              " 38: '국제>국제일반',\n",
              " 39: '문화>영화',\n",
              " 40: '경제>서비스_쇼핑',\n",
              " 41: 'IT_과학>인터넷_SNS',\n",
              " 42: '정치>행정_자치',\n",
              " 43: '경제>금융_재테크',\n",
              " 44: '국제>미국_북미',\n",
              " 45: '국제>유럽_EU',\n",
              " 46: '경제>산업_기업',\n",
              " 47: '경제>국제경제',\n",
              " 48: '사회>의료_건강',\n",
              " 49: '지역>제주',\n",
              " 50: '지역>부산',\n",
              " 51: '지역>지역일반',\n",
              " 52: '지역>충남',\n",
              " 53: 'IT_과학>IT_과학일반',\n",
              " 54: '지역>전남',\n",
              " 55: '스포츠>스포츠일반',\n",
              " 56: '국제>아시아',\n",
              " 57: '스포츠>축구>국가대표팀',\n",
              " 58: '경제>자동차',\n",
              " 59: '지역>전북',\n",
              " 60: '경제>자원',\n",
              " 61: '사회>환경',\n",
              " 62: '경제>반도체',\n",
              " 63: '지역>경기',\n",
              " 64: '스포츠>야구>메이저리그',\n",
              " 65: '지역>광주',\n",
              " 66: '스포츠>야구',\n",
              " 67: 'IT_과학>과학',\n",
              " 68: '문화>미술_건축',\n",
              " 69: '지역>강원',\n",
              " 70: '국제>중동_아프리카',\n",
              " 71: '문화>종교',\n",
              " 72: '스포츠>올림픽_아시안게임',\n",
              " 73: '사회>날씨',\n",
              " 74: '정치>선거',\n",
              " 75: '지역>울산',\n",
              " 76: '스포츠>축구',\n",
              " 77: '경제>무역',\n",
              " 78: '국제>중남미',\n",
              " 79: '국제>러시아',\n",
              " 80: '스포츠>야구>한국프로야구',\n",
              " 81: '경제>외환',\n",
              " 82: '경제>증권_증시',\n",
              " 83: '스포츠>골프',\n",
              " 84: '스포츠>축구>한국프로축구'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrmgTPCerl1b"
      },
      "source": [
        "df = df.rename(columns={'특성추출(가중치순 상위 50개)':'input'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LPEdb2tWfIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ca073c-1ec1-4901-9c30-960d44bfcb39"
      },
      "source": [
        "train, test = train, test = train_test_split(df, test_size=0.2)\n",
        "\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(48000, 4)\n",
            "(12000, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tejY9ZhABYWl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "f565cb8b-044a-4013-b3e4-e23c594ebac4"
      },
      "source": [
        "# 훈련셋의 앞부분 출력\n",
        "train.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>키워드</th>\n",
              "      <th>본문</th>\n",
              "      <th>input</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>49124</th>\n",
              "      <td>투표,트럼프,선거인단,승리시,백악관,로이터,속보,투표,트럼프,선거인단,승리시,백악관...</td>\n",
              "      <td>[속보] 트럼프 \"선거인단 투표서 바이든 승리시 백악관 떠날 것\"&lt;로이터&gt;\\n트럼프...</td>\n",
              "      <td>백악관 선거인단 로이터 미국 로이터통신 바이든 승리시 트럼프 투표 시간 현지 승리 ...</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29760</th>\n",
              "      <td>백태숙,별세,백태숙,별세,백태숙,별세,안옥순,옥선,옥자씨,기운,환경안전국장,인천,서...</td>\n",
              "      <td>▶백태숙씨 별세, 안옥순 옥선 옥자씨 기운 인천 서구청 환경안전국장 기선씨 모친상,...</td>\n",
              "      <td>부친상 발인 인천 정경부 유안타증권 우경 충청북도 서구청 이동주 훈근 부행장 백태숙...</td>\n",
              "      <td>40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50637</th>\n",
              "      <td>300만,구직자,지원,국민,취업,지원,제도,누리집,개설,취약,계층,구직자,정부,예산...</td>\n",
              "      <td>취약계층 구직자에게 정부 예산으로 1인당 월 50만원씩 6개월 동안 구직촉진수당을 ...</td>\n",
              "      <td>국민취업지원제도 구직자 구직촉진수당 수급자 누리집 사각지대 노동부 1인 6개월 bi...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59738</th>\n",
              "      <td>방한,남침,진실,수석부대변인,브라운,미국,국무부,수석,부대변인,25일,현지시간,6,...</td>\n",
              "      <td>캘 브라운 미국 국무부 수석부대변인이 25일(현지시간) 6 25전쟁 최대 격전 중 ...</td>\n",
              "      <td>중국 미국 한국 장진호 주년 70주년 유엔군 국무부 부대변인 트윗 항미원조 한미 모...</td>\n",
              "      <td>37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22182</th>\n",
              "      <td>선문대,친구,모임,감염,천안,아산,선문대,천안시,충남,아산시,모임,선문대,친구,코로...</td>\n",
              "      <td>22일 충남 천안시와 아산시에서 ‘선문대 친구 모임'과 관련된 코로나 감염자가 4명...</td>\n",
              "      <td>선문대 천안 천안시 코로나 확진자 아산시 아산 감염자 직산읍 두정동 10대 감염경로...</td>\n",
              "      <td>52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38756</th>\n",
              "      <td>공주시,활동,법인택시,노조,공주시,지부,환경,정화,공주,공주시,공주시지부,법인택시,...</td>\n",
              "      <td>[공주] 공주시와 법인택시 노조 공주시지부(지부장 최원호)는 공산성과 산성시장 일원...</td>\n",
              "      <td>법인택시 공주시 종사자 공산성 대중교통 코로나19 최원호 지부장 공주시지부 신중섭 ...</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6129</th>\n",
              "      <td>국방부,대구,공항,부지,투표,방식,의결,국방부,정경,장관,주재,대구,공항,부지,선정...</td>\n",
              "      <td>국방부는 정경두 장관 주재로 '제5회 대구 군 공항 이전부지 선정위원회'를 열어 시...</td>\n",
              "      <td>대구 위원회 참여율 경북 의성군 국방부 우보 소보 참여단 시민참여단 비안 투표참여율...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46270</th>\n",
              "      <td>이틀,연속,확진,거리,단계,격상,여부,결정,코로나,코로나,추가,신종,코로나바이러스,...</td>\n",
              "      <td>코로나 어제 569명 추가\\n\\n\\n\\n국내 신종 코로나바이러스 감염증(코로나19)...</td>\n",
              "      <td>확진자 수도권 지역발생 코로나 코로나19 감염병 인천 서울 감염증 기하급수 격리치료</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28116</th>\n",
              "      <td>마보임,스마일서브,대표,가성비,글로벌,클라우드,클라우드,사업,흑자,디지털뉴딜,공공시...</td>\n",
              "      <td>올해 코로나19 확산으로 정부주도 디지털서비스 사업이 늘면서 이를 운영하기 위한 공...</td>\n",
              "      <td>클라우드 스마일서브 경쟁력 서버리스 가성비 아이윈브이 데이터센터 마보임</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9163</th>\n",
              "      <td>강원,여성,CEO,해외,통상,진단,파견,한국여성경제인협회,강원지회,회장,이미옥,모스...</td>\n",
              "      <td>한국여성경제인협회 강원지회(회장 이미옥)는 23~28일 러시아 모스크바에 '강원 여...</td>\n",
              "      <td>강원 한국여성경제인협회 해외통상촉진단 러시아 상담회 이미옥 강원지회 통상촉 모스크바...</td>\n",
              "      <td>79</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     키워드  ... label\n",
              "49124  투표,트럼프,선거인단,승리시,백악관,로이터,속보,투표,트럼프,선거인단,승리시,백악관...  ...    44\n",
              "29760  백태숙,별세,백태숙,별세,백태숙,별세,안옥순,옥선,옥자씨,기운,환경안전국장,인천,서...  ...    40\n",
              "50637  300만,구직자,지원,국민,취업,지원,제도,누리집,개설,취약,계층,구직자,정부,예산...  ...     2\n",
              "59738  방한,남침,진실,수석부대변인,브라운,미국,국무부,수석,부대변인,25일,현지시간,6,...  ...    37\n",
              "22182  선문대,친구,모임,감염,천안,아산,선문대,천안시,충남,아산시,모임,선문대,친구,코로...  ...    52\n",
              "38756  공주시,활동,법인택시,노조,공주시,지부,환경,정화,공주,공주시,공주시지부,법인택시,...  ...    27\n",
              "6129   국방부,대구,공항,부지,투표,방식,의결,국방부,정경,장관,주재,대구,공항,부지,선정...  ...     0\n",
              "46270  이틀,연속,확진,거리,단계,격상,여부,결정,코로나,코로나,추가,신종,코로나바이러스,...  ...    27\n",
              "28116  마보임,스마일서브,대표,가성비,글로벌,클라우드,클라우드,사업,흑자,디지털뉴딜,공공시...  ...    46\n",
              "9163   강원,여성,CEO,해외,통상,진단,파견,한국여성경제인협회,강원지회,회장,이미옥,모스...  ...    79\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgjMzosCDD35"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# **Preprocessing - TRAINING SET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GoESQ0jbybJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1337e1-0594-499b-a242-d63b99609268"
      },
      "source": [
        "# 리뷰 문장 추출\n",
        "sentences = train['input']\n",
        "sentences[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49124    백악관 선거인단 로이터 미국 로이터통신 바이든 승리시 트럼프 투표 시간 현지 승리 ...\n",
              "29760    부친상 발인 인천 정경부 유안타증권 우경 충청북도 서구청 이동주 훈근 부행장 백태숙...\n",
              "50637    국민취업지원제도 구직자 구직촉진수당 수급자 누리집 사각지대 노동부 1인 6개월 bi...\n",
              "59738    중국 미국 한국 장진호 주년 70주년 유엔군 국무부 부대변인 트윗 항미원조 한미 모...\n",
              "22182    선문대 천안 천안시 코로나 확진자 아산시 아산 감염자 직산읍 두정동 10대 감염경로...\n",
              "38756    법인택시 공주시 종사자 공산성 대중교통 코로나19 최원호 지부장 공주시지부 신중섭 ...\n",
              "6129     대구 위원회 참여율 경북 의성군 국방부 우보 소보 참여단 시민참여단 비안 투표참여율...\n",
              "46270       확진자 수도권 지역발생 코로나 코로나19 감염병 인천 서울 감염증 기하급수 격리치료\n",
              "28116              클라우드 스마일서브 경쟁력 서버리스 가성비 아이윈브이 데이터센터 마보임\n",
              "9163     강원 한국여성경제인협회 해외통상촉진단 러시아 상담회 이미옥 강원지회 통상촉 모스크바...\n",
              "Name: input, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KkJZvhccRUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1c7c3b-d74c-48ae-f40b-174c4041ce9a"
      },
      "source": [
        "# BERT의 입력 형식에 맞게 변환\n",
        "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
        "sentences[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] 백악관 선거인단 로이터 미국 로이터통신 바이든 승리시 트럼프 투표 시간 현지 승리 속보 대통령 보도 [SEP]',\n",
              " '[CLS] 부친상 발인 인천 정경부 유안타증권 우경 충청북도 서구청 이동주 훈근 부행장 백태숙 경용 제천서울병원 이대서울병원 신촌세브란스병원 김현덕 이호무 모친상 강현대씨 강현대 기선씨 장모상 팀장 나영 옥자씨 강민 [SEP]',\n",
              " '[CLS] 국민취업지원제도 구직자 구직촉진수당 수급자 누리집 사각지대 노동부 1인 6개월 bi 40만 원씩 저소득층 300만 한국 제2 [SEP]',\n",
              " '[CLS] 중국 미국 한국 장진호 주년 70주년 유엔군 국무부 부대변인 트윗 항미원조 한미 모건 행정부 대변인 마오쩌둥 9만 피난민 [SEP]',\n",
              " '[CLS] 선문대 천안 천안시 코로나 확진자 아산시 아산 감염자 직산읍 두정동 10대 감염경로 충남 시민들 기숙사 방역수칙 양성판정 관계자 야유회 422번 확진판정 [SEP]',\n",
              " '[CLS] 법인택시 공주시 종사자 공산성 대중교통 코로나19 최원호 지부장 공주시지부 신중섭 지원방안 교통과장 직원들 지역민 공주시민 감사인사 구슬땀 운수종사자들 활동 감사 공주 일원 행사 주변 상황 일대 공산 산성 시장 지부 환경 정화 노조 [SEP]',\n",
              " '[CLS] 대구 위원회 참여율 경북 의성군 국방부 우보 소보 참여단 시민참여단 비안 투표참여율 후보지 부지 투표 선정 방식 의결 시민 지역 장관 공항 군위 합산 의견 투명 절차 공정 노력 [SEP]',\n",
              " '[CLS] 확진자 수도권 지역발생 코로나 코로나19 감염병 인천 서울 감염증 기하급수 격리치료 [SEP]',\n",
              " '[CLS] 클라우드 스마일서브 경쟁력 서버리스 가성비 아이윈브이 데이터센터 마보임 [SEP]',\n",
              " '[CLS] 강원 한국여성경제인협회 해외통상촉진단 러시아 상담회 이미옥 강원지회 통상촉 모스크바 ceo 파견 해외 현지 진단 수출 사진 여성 회장 바이어 제공 통상 [SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hBblIVQcXJR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b1155cb-c829-4d85-b1e8-755a79c922c4"
      },
      "source": [
        "# 라벨 추출\n",
        "labels = train['label'].values\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([44, 40,  2, ..., 40, 23, 23])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwEplfDvcnZG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "b1b45979a2284a5f9dfb0c5deff7f6f9",
            "f2ca53a2761f490f8af6e091f6d5b687",
            "930246900a094212b40226eca2b8fd89",
            "2fc0c41ce9e64f92ad1222fce80c931c",
            "f1698d43b1334fb7930669dae1fcb40f",
            "da1526e9b5014a41b1e03799e045e4dc",
            "286be3173a0f41a680f9d1bef8978845",
            "f327bb1ab7d34851b32e40d056b8e42e"
          ]
        },
        "outputId": "e112c568-12ec-4f19-cb7c-7561c09a6c6e"
      },
      "source": [
        "# BERT의 토크나이저로 문장을 토큰으로 분리\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "print (sentences[0])\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1b45979a2284a5f9dfb0c5deff7f6f9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[CLS] 백악관 선거인단 로이터 미국 로이터통신 바이든 승리시 트럼프 투표 시간 현지 승리 속보 대통령 보도 [SEP]\n",
            "['[CLS]', '백', '##악', '##관', '선', '##거', '##인', '##단', '로', '##이터', '미국', '로', '##이터', '##통', '##신', '바', '##이', '##든', '승', '##리', '##시', '트', '##럼', '##프', '투', '##표', '시', '##간', '현', '##지', '승', '##리', '속', '##보', '대통령', '보', '##도', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "QQf6JsMhiVWW",
        "outputId": "35b28fe1-903f-410b-fb0c-978d0bdca2ef"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(list(map(len, tokenized_texts)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD6CAYAAABDPiuvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAUZ0lEQVR4nO3df6zd9X3f8edruNAmbWIDt4za1q7XOJkctCX0Djyli5qw2QaqmklpZDYNL7NqaXW6dIuWmEaaqyRIZD/KgtpQucHDRBEOoumwFlLqEjo0qfy4BAIYh3ADJL6WwTcxkHZRyUze++N8vByu7+Xee871Pebe50M6Ot/v+/v5nu/n+9GxX/f745yTqkKStLT9rUF3QJI0eIaBJMkwkCQZBpIkDANJEoaBJIlZhEGSPUmOJXliUv23knwzycEk/6mrfm2SsSRPJdnYVd/UamNJdnbV1yR5oNW/lOTs+do5SdLsZKbPGSR5L/DXwK1VdVGrvQ/4BHBlVb2S5Oer6liSdcBtwCXALwB/Dry9vdS3gH8KjAMPAVdX1ZNJbge+XFX7kvwh8I2qummmjp9//vk1PDw89z2WpCXs4Ycf/l5VDU2uL5tpxaq6L8nwpPK/Aa6vqldam2OtvhnY1+rPJhmjEwwAY1X1DECSfcDmJIeA9wP/vLXZC/wuMGMYDA8PMzo6OlMzSVKXJN+Zqt7rNYO3A/+4nd75X0n+YauvBA53tRtvtenq5wEvVdWJSfUpJdmeZDTJ6MTERI9dlyRN1msYLAPOBdYD/wG4PUnmrVfTqKrdVTVSVSNDQ6cc5UiSejTjaaJpjNM5z1/Ag0l+DJwPHAFWd7Vb1WpMU/8+sDzJsnZ00N1ekrRAej0y+B/A+wCSvB04G/gesB/YkuScJGuAtcCDdC4Yr213Dp0NbAH2tzC5F/hAe92twJ297owkqTczHhkkuQ34FeD8JOPALmAPsKfdbvojYGv7j/1guzvoSeAEsKOqXm2v82HgbuAsYE9VHWyb+DiwL8mngUeAm+dx/yRJszDjraVnqpGRkfJuIkmamyQPV9XI5LqfQJYkGQaSJMNAkkTvt5ZKszK88ysD2/Zz1185sG1LbzQeGUiSDANJkmEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAk4RfVLRmD/MI4SWc+jwwkSTOHQZI9SY613zuevOyjSSrJ+W0+SW5MMpbksSQXd7XdmuTp9tjaVf+lJI+3dW5MkvnaOUnS7MzmyOAWYNPkYpLVwAbgu13ly4G17bEduKm1PRfYBVwKXALsSrKirXMT8Btd652yLUnS6TVjGFTVfcDxKRbdAHwMqK7aZuDW6rgfWJ7kQmAjcKCqjlfVi8ABYFNb9paqur+qCrgVuKq/XZIkzVVP1wySbAaOVNU3Ji1aCRzumh9vtderj09Rn26725OMJhmdmJjopeuSpCnMOQySvAn4HeA/zn93Xl9V7a6qkaoaGRoaWujNS9Ki1cuRwS8Ca4BvJHkOWAV8PcnfBo4Aq7varmq116uvmqIuSVpAcw6Dqnq8qn6+qoarapjOqZ2Lq+p5YD9wTburaD3wclUdBe4GNiRZ0S4cbwDubst+kGR9u4voGuDOedo3SdIszebW0tuAvwTekWQ8ybbXaX4X8AwwBvwR8JsAVXUc+BTwUHt8stVobT7f1vk28NXedkWS1KsZP4FcVVfPsHy4a7qAHdO02wPsmaI+Clw0Uz8kSaePn0CWJBkGkiTDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkidn9BvKeJMeSPNFV+89JvpnksSR/kmR517Jrk4wleSrJxq76plYbS7Kzq74myQOt/qUkZ8/nDkqSZjabI4NbgE2TageAi6rq7wPfAq4FSLIO2AK8s63zuSRnJTkL+APgcmAdcHVrC/AZ4IaqehvwIrCtrz2SJM3ZjGFQVfcBxyfV/qyqTrTZ+4FVbXozsK+qXqmqZ4Ex4JL2GKuqZ6rqR8A+YHOSAO8H7mjr7wWu6nOfJElzNB/XDP418NU2vRI43LVsvNWmq58HvNQVLCfrU0qyPcloktGJiYl56LokCfoMgySfAE4AX5yf7ry+qtpdVSNVNTI0NLQQm5SkJWFZrysm+VfArwKXVVW18hFgdVezVa3GNPXvA8uTLGtHB93tJUkLpKcjgySbgI8Bv1ZVP+xatB/YkuScJGuAtcCDwEPA2nbn0Nl0LjLvbyFyL/CBtv5W4M7edkWS1KvZ3Fp6G/CXwDuSjCfZBvw+8HPAgSSPJvlDgKo6CNwOPAn8KbCjql5tf/V/GLgbOATc3toCfBz490nG6FxDuHle91CSNKMZTxNV1dVTlKf9D7uqrgOum6J+F3DXFPVn6NxtJEkaED+BLEkyDCRJhoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKz+w3kPUmOJXmiq3ZukgNJnm7PK1o9SW5MMpbksSQXd62ztbV/OsnWrvovJXm8rXNjksz3TkqSXt9sjgxuATZNqu0E7qmqtcA9bR7gcmBte2wHboJOeAC7gEvp/N7xrpMB0tr8Rtd6k7clSTrNZgyDqroPOD6pvBnY26b3Ald11W+tjvuB5UkuBDYCB6rqeFW9CBwANrVlb6mq+6uqgFu7XkuStEB6vWZwQVUdbdPPAxe06ZXA4a524632evXxKepTSrI9yWiS0YmJiR67LkmarO8LyO0v+pqHvsxmW7uraqSqRoaGhhZik5K0JPQaBi+0Uzy052OtfgRY3dVuVau9Xn3VFHVJ0gLqNQz2AyfvCNoK3NlVv6bdVbQeeLmdTrob2JBkRbtwvAG4uy37QZL17S6ia7peS5K0QJbN1CDJbcCvAOcnGadzV9D1wO1JtgHfAT7Ymt8FXAGMAT8EPgRQVceTfAp4qLX7ZFWdvCj9m3TuWPoZ4KvtIUlaQDOGQVVdPc2iy6ZoW8COaV5nD7BnivoocNFM/ZAknT5+AlmSZBhIkgwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIk+gyDJP8uycEkTyS5LclPJ1mT5IEkY0m+lOTs1vacNj/Wlg93vc61rf5Uko397ZIkaa56DoMkK4F/C4xU1UXAWcAW4DPADVX1NuBFYFtbZRvwYqvf0NqRZF1b753AJuBzSc7qtV+SpLnr9zTRMuBnkiwD3gQcBd4P3NGW7wWuatOb2zxt+WVJ0ur7quqVqnoWGAMu6bNfkqQ56DkMquoI8F+A79IJgZeBh4GXqupEazYOrGzTK4HDbd0Trf153fUp1nmNJNuTjCYZnZiY6LXrkqRJ+jlNtILOX/VrgF8A3kznNM9pU1W7q2qkqkaGhoZO56YkaUlZ1se6/wR4tqomAJJ8GXgPsDzJsvbX/yrgSGt/BFgNjLfTSm8Fvt9VP6l7Halnwzu/MpDtPnf9lQPZrtSPfq4ZfBdYn+RN7dz/ZcCTwL3AB1qbrcCdbXp/m6ct/1pVVatvaXcbrQHWAg/20S9J0hz1fGRQVQ8kuQP4OnACeATYDXwF2Jfk0612c1vlZuALScaA43TuIKKqDia5nU6QnAB2VNWrvfZLkjR3/Zwmoqp2AbsmlZ9hiruBqupvgF+f5nWuA67rpy+SpN75CWRJUn9HBpqbQV3QlKSZeGQgSTIMJEmGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEn0GQZJlie5I8k3kxxK8o+SnJvkQJKn2/OK1jZJbkwyluSxJBd3vc7W1v7pJFv73SlJ0tz0e2TwWeBPq+rvAf8AOATsBO6pqrXAPW0e4HJgbXtsB24CSHIund9RvpTObyfvOhkgkqSF0XMYJHkr8F7gZoCq+lFVvQRsBva2ZnuBq9r0ZuDW6rgfWJ7kQmAjcKCqjlfVi8ABYFOv/ZIkzV0/RwZrgAngvyd5JMnnk7wZuKCqjrY2zwMXtOmVwOGu9cdbbbr6KZJsTzKaZHRiYqKPrkuSuvUTBsuAi4GbqurdwP/hJ6eEAKiqAqqPbbxGVe2uqpGqGhkaGpqvl5WkJa+fMBgHxqvqgTZ/B51weKGd/qE9H2vLjwCru9Zf1WrT1SVJC6TnMKiq54HDSd7RSpcBTwL7gZN3BG0F7mzT+4Fr2l1F64GX2+mku4ENSVa0C8cbWk2StECW9bn+bwFfTHI28AzwIToBc3uSbcB3gA+2tncBVwBjwA9bW6rqeJJPAQ+1dp+squN99kuSNAd9hUFVPQqMTLHosinaFrBjmtfZA+zppy+SpN75CWRJkmEgSTIMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSmIcwSHJWkkeS/M82vybJA0nGknyp/T4ySc5p82Nt+XDXa1zb6k8l2dhvnyRJczMfRwYfAQ51zX8GuKGq3ga8CGxr9W3Ai61+Q2tHknXAFuCdwCbgc0nOmod+SZJmqa8wSLIKuBL4fJsP8H7gjtZkL3BVm97c5mnLL2vtNwP7quqVqnoWGAMu6adfkqS56ffI4L8BHwN+3ObPA16qqhNtfhxY2aZXAocB2vKXW/v/X59inddIsj3JaJLRiYmJPrsuSTqp5zBI8qvAsap6eB7787qqandVjVTVyNDQ0EJtVpIWvWV9rPse4NeSXAH8NPAW4LPA8iTL2l//q4Ajrf0RYDUwnmQZ8Fbg+131k7rXkSQtgJ6PDKrq2qpaVVXDdC4Af62q/gVwL/CB1mwrcGeb3t/macu/VlXV6lva3UZrgLXAg732S5I0d/0cGUzn48C+JJ8GHgFubvWbgS8kGQOO0wkQqupgktuBJ4ETwI6qevU09EuSNI15CYOq+gvgL9r0M0xxN1BV/Q3w69Osfx1w3Xz0RZI0d34CWZJkGEiSDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiT6CIMkq5Pcm+TJJAeTfKTVz01yIMnT7XlFqyfJjUnGkjyW5OKu19ra2j+dZGv/uyVJmot+jgxOAB+tqnXAemBHknXATuCeqloL3NPmAS4H1rbHduAm6IQHsAu4lM5vJ+86GSCSpIXRcxhU1dGq+nqb/ivgELAS2Azsbc32Ale16c3ArdVxP7A8yYXARuBAVR2vqheBA8CmXvslSZq7eblmkGQYeDfwAHBBVR1ti54HLmjTK4HDXauNt9p09am2sz3JaJLRiYmJ+ei6JIl5CIMkPwv8MfDbVfWD7mVVVUD1u42u19tdVSNVNTI0NDRfLytJS15fYZDkp+gEwRer6sut/EI7/UN7PtbqR4DVXauvarXp6pKkBdLP3UQBbgYOVdXvdS3aD5y8I2grcGdX/Zp2V9F64OV2OuluYEOSFe3C8YZWkyQtkGV9rPse4F8Cjyd5tNV+B7geuD3JNuA7wAfbsruAK4Ax4IfAhwCq6niSTwEPtXafrKrjffRLkjRHPYdBVf1vINMsvmyK9gXsmOa19gB7eu2LJKk/fgJZktTXaSJJUxje+ZWBbfu5668c2Lb1xuaRgSTJMJAkGQaSJAwDSRJL9ALyIC/wSdKZyCMDSZJhIEkyDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSZxB302UZBPwWeAs4PNVdf2AuyS94Qzqe7f8UZ03vjPiyCDJWcAfAJcD64Crk6wbbK8kaek4U44MLgHGquoZgCT7gM3AkwPtlaRZWYrfBLzYjobOlDBYCRzumh8HLp3cKMl2YHub/eskT83y9c8HvtdXDxcXx+NUjslrOR6nes2Y5DMD7El//s5UxTMlDGalqnYDu+e6XpLRqho5DV16Q3I8TuWYvJbjcarFPiZnxDUD4Aiwumt+VatJkhbAmRIGDwFrk6xJcjawBdg/4D5J0pJxRpwmqqoTST4M3E3n1tI9VXVwHjcx51NLi5zjcSrH5LUcj1Mt6jFJVQ26D5KkATtTThNJkgbIMJAkLe4wSLIpyVNJxpLsHHR/BiXJc0keT/JoktFWOzfJgSRPt+cVg+7n6ZRkT5JjSZ7oqk05Bum4sb1vHkty8eB6fnpMMx6/m+RIe588muSKrmXXtvF4KsnGwfT69EmyOsm9SZ5McjDJR1p9ybxHFm0Y+BUXp3hfVb2r6z7pncA9VbUWuKfNL2a3AJsm1aYbg8uBte2xHbhpgfq4kG7h1PEAuKG9T95VVXcBtH83W4B3tnU+1/59LSYngI9W1TpgPbCj7feSeY8s2jCg6ysuqupHwMmvuFDHZmBvm94LXDXAvpx2VXUfcHxSebox2AzcWh33A8uTXLgwPV0Y04zHdDYD+6rqlap6Fhij8+9r0aiqo1X19Tb9V8AhOt+MsGTeI4s5DKb6iouVA+rLoBXwZ0kebl/pAXBBVR1t088DFwymawM13Rgs5ffOh9tpjz1dpw6X1HgkGQbeDTzAEnqPLOYw0E/8clVdTOfQdkeS93YvrM79xUv6HmPHAOic6vhF4F3AUeC/DrY7Cy/JzwJ/DPx2Vf2ge9lif48s5jDwKy6aqjrSno8Bf0LnEP+Fk4e17fnY4Ho4MNONwZJ871TVC1X1alX9GPgjfnIqaEmMR5KfohMEX6yqL7fyknmPLOYw8CsugCRvTvJzJ6eBDcATdMZia2u2FbhzMD0cqOnGYD9wTbtjZD3wctepgkVr0jnvf0bnfQKd8diS5Jwka+hcNH1woft3OiUJcDNwqKp+r2vR0nmPVNWifQBXAN8Cvg18YtD9GdAY/F3gG+1x8OQ4AOfRuTviaeDPgXMH3dfTPA630Tn18X/pnN/dNt0YAKFzJ9q3gceBkUH3f4HG4wttfx+j85/dhV3tP9HG4yng8kH3/zSMxy/TOQX0GPBoe1yxlN4jfh2FJGlRnyaSJM2SYSBJMgwkSYaBJAnDQJKEYSBJwjCQJAH/D9yESSj2zDzcAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ76KiP_dLn-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e796d5b-d6ad-4a24-883d-5e7c7f5ffdfb"
      },
      "source": [
        "# 입력 토큰의 최대 시퀀스 길이\n",
        "MAX_LEN = 128\n",
        "\n",
        "# 토큰을 숫자 인덱스로 변환\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\n",
        "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "input_ids[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   101,   9331, 119110,  20595,   9428,  41521,  12030,  24989,\n",
              "         9202,  85297,  23545,   9202,  85297,  43022,  25387,   9318,\n",
              "        10739,  90537,   9484,  12692,  14040,   9890, 118866,  28396,\n",
              "         9881,  37824,   9485,  18784,   9978,  12508,   9484,  12692,\n",
              "         9449,  30005,  70672,   9356,  12092,    102,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y10yOyYFrj90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2430ff80-1613-4c5a-9271-775c895a1289"
      },
      "source": [
        "print(tokenizer.convert_ids_to_tokens(input_ids[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', '백', '##악', '##관', '선', '##거', '##인', '##단', '로', '##이터', '미국', '로', '##이터', '##통', '##신', '바', '##이', '##든', '승', '##리', '##시', '트', '##럼', '##프', '투', '##표', '시', '##간', '현', '##지', '승', '##리', '속', '##보', '대통령', '보', '##도', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKfL8SotdVaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e2764a-d8ac-4fdc-b64a-73aed6ac823c"
      },
      "source": [
        "# 어텐션 마스크 초기화\n",
        "attention_masks = []\n",
        "\n",
        "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "print(attention_masks[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f5Vq3-7eNKH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0258e8be-018d-41a4-8e21-d98d36168457"
      },
      "source": [
        "# 훈련셋과 검증셋으로 분리\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
        "                                                                                    labels, \n",
        "                                                                                    random_state=2020, \n",
        "                                                                                    test_size=0.1)\n",
        "\n",
        "# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
        "                                                       input_ids,\n",
        "                                                       random_state=2020,\n",
        "                                                       test_size=0.1)\n",
        "\n",
        "# 데이터를 파이토치의 텐서로 변환\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "validation_masks = torch.tensor(validation_masks)\t\t\t\t\n",
        "\n",
        "print(train_inputs[0])\n",
        "print(train_labels[0])\n",
        "print(train_masks[0])\n",
        "print(validation_inputs[0])\n",
        "print(validation_labels[0])\n",
        "print(validation_masks[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([   101,   9365,  21386,   9460,  86834,  24017,   9812,  11261,  16439,\n",
            "          9641,  16323,  18392,  18784,   9641,  16323,  18392,   9485,  86834,\n",
            "         13890,   9485,  25242,  83811,  40311,   9651,  11287,  45465,  12692,\n",
            "          9612,  45465,  15891,  26784,   9812,  11261,  16439,  54055,   9994,\n",
            "         21386,  24982,   9953,  24017,  27023,   8907, 119081,  22333,    102,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0])\n",
            "tensor(3)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "tensor([   101, 103988,  14040,   9670,  14523,  25242,   9069, 119187,  21611,\n",
            "        103155,   9953,  51945,  25549,  12945,   8848, 119144,  73380,    126,\n",
            "         22200,   9356,  71439,  70915,  12508,  20479,  13890,   9966,  12945,\n",
            "          8863,  12692,  58939,   9814,  12424,  15184,   9711,  33188,   9056,\n",
            "         15891,   9059,  29683,   9735,  40958,   9981,  20626,   9485,  36553,\n",
            "          9485,  25549,   9730,  24989,   9069, 119187,   8887, 103155,   9485,\n",
            "         19855,   9711,  14863,   9691,  29455,   9625,  58303,   8909,  13890,\n",
            "          9414,  79544,   9357,  12508,   9665,  20479,   9069,  14871,   9766,\n",
            "         17730,   9067,  20479,   8878,  14871,   9367,  21711,   9059,  21611,\n",
            "          9328, 119285,   9625,  12945,  14040,   9678,  18622,   9059,  19855,\n",
            "          9689,  20595,   8922,  17730,   9356,  71439,   9953,  51945,   9640,\n",
            "         14279,    102,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0])\n",
            "tensor(9)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3vlyUJuVRo5"
      },
      "source": [
        "# 배치 사이즈\n",
        "batch_size = 32\n",
        "\n",
        "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
        "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkqUHx51dffp"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# **Preprocessing - TEST SET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgrsNuArd4pj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3742f48e-bcef-472b-b320-af19b820af38"
      },
      "source": [
        "# 리뷰 문장 추출\n",
        "sentences = test['input']\n",
        "sentences[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31813    공무원 환경부 정부세종청사 코로나19 해양수산 세종청사 세종 확진자 청사 확진 정부...\n",
              "52486    직불금 진천군 미이행 115억 만큼 공익적 서정배 코로나19 농업인 4차례 4500...\n",
              "21498     코로나19 대유행 미국 확진자 코로나 소프트뱅크 유럽 팬데믹 금융위기 한국 반도체 각국\n",
              "15145    충주 교통사고 교통질서 충주경찰서 시민들 교통문화 정재일 안전띠 수요일 이륜차안전모...\n",
              "48009    백종원 소유진 맛남 유병재 용희 요섹남 서현 1남 물고추 다둥 이듬해 오른쪽 고춧가...\n",
              "25169    기아차 중국 셀토스 1위 suv 자동차 쏘넷 기아자동차 영업이익 미국 4위 삼성증권 1인\n",
              "7429     중국 추궈홍 비핵화 관련국 중국대사 전환기 김무성 미국 의원회관 한중 잠재력 김정은...\n",
              "30971    외부인 코로나19 서울 코로나바이러스 연세대학교 안내문 신촌 대학가 감염증 서대문구...\n",
              "12665    음악회 원어민교사 진천여중 주무관 수요일 교직원 학생들 데영 김희선 원어민 성원 관...\n",
              "59504    경찰대 코로나19 한국 독일 금융사기 https 영국 성범죄 유튜브 코로나 홈페이지...\n",
              "Name: input, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtz3QZt9d4pz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f72120d7-e585-4893-a316-712110f5fd3c"
      },
      "source": [
        "# BERT의 입력 형식에 맞게 변환\n",
        "sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in sentences]\n",
        "sentences[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS] 공무원 환경부 정부세종청사 코로나19 해양수산 세종청사 세종 확진자 청사 확진 정부 국장 근무 폐쇄 판정 무더기 제외 전원 인원 귀가 필수 조치 [SEP]',\n",
              " '[CLS] 직불금 진천군 미이행 115억 만큼 공익적 서정배 코로나19 농업인 4차례 4500만 농가들 농업기술센터소장 진천 역진 지급 농가 단가 공익 미만 면적 [SEP]',\n",
              " '[CLS] 코로나19 대유행 미국 확진자 코로나 소프트뱅크 유럽 팬데믹 금융위기 한국 반도체 각국 [SEP]',\n",
              " '[CLS] 충주 교통사고 교통질서 충주경찰서 시민들 교통문화 정재일 안전띠 수요일 이륜차안전모 이륜차 70여 파출소 안전모 신호위반 법원사 음주운전 지구대 홍보활동 법원사거리 연말연시 교통경찰 10개소 첫날 교통 운전 운영 단속 사용 경찰 증가 정착 [SEP]',\n",
              " '[CLS] 백종원 소유진 맛남 유병재 용희 요섹남 서현 1남 물고추 다둥 이듬해 오른쪽 고춧가루 방송인 sbs 김치맛 방송화면 인스타그램 파김치 와이프 요리 방송 양파 섹시 중간 사진 아들 배우 광장 상식 아내 예능 슬하 [SEP]',\n",
              " '[CLS] 기아차 중국 셀토스 1위 suv 자동차 쏘넷 기아자동차 영업이익 미국 4위 삼성증권 1인 [SEP]',\n",
              " '[CLS] 중국 추궈홍 비핵화 관련국 중국대사 전환기 김무성 미국 의원회관 한중 잠재력 김정은 지도자 서울 [SEP]',\n",
              " '[CLS] 외부인 코로나19 서울 코로나바이러스 연세대학교 안내문 신촌 대학가 감염증 서대문구 출입 신종 캠퍼스 중심 감염 예방 설치 [SEP]',\n",
              " '[CLS] 음악회 원어민교사 진천여중 주무관 수요일 교직원 학생들 데영 김희선 원어민 성원 관객들 연주자 시청각실 등대지기 알레드 로렌 이너프 권진아 삽입곡 쇼맨 댄스무대 댄스팀 해금 양승준 시설관리주무관 학교구성원들 감동적 최경희 구성원 콜라보 7회 성료 진천여 제7회 deyoung [SEP]',\n",
              " '[CLS] 경찰대 코로나19 한국 독일 금융사기 https 영국 성범죄 유튜브 코로나 홈페이지 국제학술세미나 실시간 학술세미나 다크웹 대응책 2020knpuias 범죄 치안 세미나 대응 진행 주제 경찰 미래 전화 학술 동향 개최 여성 포스트 공동 [SEP]']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li8oRajbd4p3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4248189c-1932-4201-b995-04dd7e87676a"
      },
      "source": [
        "# 라벨 추출\n",
        "labels = test['label'].values\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9, 43, 47, ..., 61, 60, 15])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvpQ49nEd4p6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17b23c2a-1db3-4a62-c810-e737a64dbe40"
      },
      "source": [
        "# BERT의 토크나이저로 문장을 토큰으로 분리\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "print (sentences[0])\n",
        "print (tokenized_texts[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] 공무원 환경부 정부세종청사 코로나19 해양수산 세종청사 세종 확진자 청사 확진 정부 국장 근무 폐쇄 판정 무더기 제외 전원 인원 귀가 필수 조치 [SEP]\n",
            "['[CLS]', '공', '##무', '##원', '환', '##경', '##부', '정', '##부', '##세', '##종', '##청', '##사', '코', '##로', '##나', '##19', '해', '##양', '##수', '##산', '세', '##종', '##청', '##사', '세', '##종', '확', '##진', '##자', '청', '##사', '확', '##진', '정', '##부', '국', '##장', '근', '##무', '폐', '##쇄', '판', '##정', '무', '##더', '##기', '제', '##외', '전', '##원', '인', '##원', '귀', '##가', '필', '##수', '조', '##치', '[SEP]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HI9viuAvd4p_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c99fdbeb-08f7-417b-d7f7-9c466dde5bee"
      },
      "source": [
        "# 입력 토큰의 최대 시퀀스 길이\n",
        "MAX_LEN = 128\n",
        "\n",
        "# 토큰을 숫자 인덱스로 변환\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "\n",
        "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "input_ids[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   101,   8896,  32537,  14279,   9995,  31720,  14646,   9670,\n",
              "        14646,  24982,  22200,  40311,  12945,   9812,  11261,  16439,\n",
              "        54055,   9960,  37114,  15891,  21386,   9435,  22200,  40311,\n",
              "        12945,   9435,  22200,   9994,  18623,  13764,   9751,  12945,\n",
              "         9994,  18623,   9670,  14646,   8909,  13890,   8926,  32537,\n",
              "         9927, 119058,   9903,  16605,   9294,  54141,  12310,   9672,\n",
              "        78705,   9665,  14279,   9640,  14279,   8920,  11287,   9949,\n",
              "        15891,   9678,  18622,    102,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1NKmP0Fd4qD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bca6820-841b-4b3b-c671-2666e6660118"
      },
      "source": [
        "# 어텐션 마스크 초기화\n",
        "attention_masks = []\n",
        "\n",
        "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "print(attention_masks[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIkaYCGbd4qG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33682589-6219-4b67-8d8d-5027cec270f2"
      },
      "source": [
        "# 데이터를 파이토치의 텐서로 변환\n",
        "test_inputs = torch.tensor(input_ids)\n",
        "test_labels = torch.tensor(labels)\n",
        "test_masks = torch.tensor(attention_masks)\n",
        "\n",
        "print(test_inputs[0])\n",
        "print(test_labels[0])\n",
        "print(test_masks[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([   101,   8896,  32537,  14279,   9995,  31720,  14646,   9670,  14646,\n",
            "         24982,  22200,  40311,  12945,   9812,  11261,  16439,  54055,   9960,\n",
            "         37114,  15891,  21386,   9435,  22200,  40311,  12945,   9435,  22200,\n",
            "          9994,  18623,  13764,   9751,  12945,   9994,  18623,   9670,  14646,\n",
            "          8909,  13890,   8926,  32537,   9927, 119058,   9903,  16605,   9294,\n",
            "         54141,  12310,   9672,  78705,   9665,  14279,   9640,  14279,   8920,\n",
            "         11287,   9949,  15891,   9678,  18622,    102,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0])\n",
            "tensor(9)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gwdYv1Ad4qK"
      },
      "source": [
        "# 배치 사이즈\n",
        "batch_size = 32\n",
        "\n",
        "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
        "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBvpU-Hfgcth"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# **MODEL Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heToD1ev0mOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c350ba1-a71c-4a1b-a31c-92a59664d470"
      },
      "source": [
        "# GPU 디바이스 이름 구함\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# GPU 디바이스 이름 검사\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6enIxvt1FB2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd76d96a-5e4f-49fd-9316-d114758ca628"
      },
      "source": [
        "# 디바이스 설정\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla V100-SXM2-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS2MXSiLg5zC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4518bb6d20e541daa887bed71409a483",
            "77eeb47af0214919b714618610b5ca82",
            "244b95b22a4e455c8a57fbe2fac72cb9",
            "5a94a62da0c84425bbb6d40895f27c6d",
            "b4a940374d8e412e8863f611408398ea",
            "bea8470a63214076a835572f0c37a602",
            "c1253b9fc46b4f0791dbfc5ede9a93b9",
            "2ba0dd743d594330a7d750d38375ae2c",
            "768094f521194f0c984b67731106ff55",
            "e87c5d9386104331b6f5c3bbc10f86aa",
            "041e5859104a4c6fb8941a667d38e40f",
            "9b62bcc498b445959ccb35d48f974a0d",
            "a577060c071342ef8a1079b3381a5b6f",
            "572f42454022428c884e3169bb07d858",
            "0d0bba95850f4b719b392d4fb9dba380",
            "f31c351e7c0541de83bacd4b301a6417"
          ]
        },
        "outputId": "6928e57e-a9aa-4ab4-c291-7b3974e75a3b"
      },
      "source": [
        "# 분류를 위한 BERT 모델 생성\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=NUM_LABELS)\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4518bb6d20e541daa887bed71409a483",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "768094f521194f0c984b67731106ff55",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=85, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIdfbLTuWmxk"
      },
      "source": [
        "# 옵티마이저 설정\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # 학습률\n",
        "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
        "                )\n",
        "\n",
        "# 에폭수\n",
        "epochs = 30\n",
        "\n",
        "# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# 처음에 학습률을 조금씩 변화시키는 스케줄러 생성\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzCHV_ghj7DM"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# **MODEL Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0-p6pPVXCRe"
      },
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJXISnJzCdLM"
      },
      "source": [
        "def format_time(elapsed):\n",
        "\n",
        "    # 반올림\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # hh:mm:ss으로 형태 변경\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aum0P-dbjOyE"
      },
      "source": [
        "from time import gmtime, strftime\n",
        "START_TIME = strftime(\"%Y_%m_%d_%H_%M_%S\", gmtime())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muU2kS2GCh4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d3b6dbe-ea27-495b-fd26-4123c9c1ffed"
      },
      "source": [
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "model.zero_grad()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # 시작 시간 설정\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 로스 초기화\n",
        "    total_loss = 0\n",
        "\n",
        "    # 훈련모드로 변경\n",
        "    model.train()\n",
        "        \n",
        "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # 경과 정보 표시\n",
        "        if step % 500 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # 배치를 GPU에 넣음\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # 배치에서 데이터 추출\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Forward 수행                \n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "        \n",
        "        # 로스 구함\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # 총 로스 계산\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward 수행으로 그래디언트 계산\n",
        "        loss.backward()\n",
        "\n",
        "        # 그래디언트 클리핑\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 스케줄러로 학습률 감소\n",
        "        scheduler.step()\n",
        "\n",
        "        # 그래디언트 초기화\n",
        "        model.zero_grad()\n",
        "\n",
        "    # 평균 로스 계산\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "    \n",
        "    PATH = BASE_URL+\"/bert_news_model_{}.pth\".format(START_TIME)\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "    print(\"  Model saved at {}\".format(PATH))\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    #시작 시간 설정\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 평가모드로 변경\n",
        "    model.eval()\n",
        "\n",
        "    # 변수 초기화\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "    for batch in validation_dataloader:\n",
        "        # 배치를 GPU에 넣음\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # 배치에서 데이터 추출\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # 그래디언트 계산 안함\n",
        "        with torch.no_grad():     \n",
        "            # Forward 수행\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # 로스 구함\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # CPU로 데이터 이동\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:58.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:56.\n",
            "\n",
            "  Average training loss: 2.83\n",
            "  Training epcoh took: 0:05:19\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.43\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 2 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:58.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:56.\n",
            "\n",
            "  Average training loss: 2.06\n",
            "  Training epcoh took: 0:05:18\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.51\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 3 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:58.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:56.\n",
            "\n",
            "  Average training loss: 1.72\n",
            "  Training epcoh took: 0:05:19\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.54\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 4 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 1.47\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.54\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 5 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 1.27\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.57\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 6 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 1.09\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.57\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 7 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 0.93\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.57\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 8 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 0.78\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.57\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 9 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:58.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:56.\n",
            "\n",
            "  Average training loss: 0.66\n",
            "  Training epcoh took: 0:05:19\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.58\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 10 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.57\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 11 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 0.45\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.57\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 12 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:58.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.58\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 13 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:58.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 0.30\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.57\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 14 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 0.24\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.58\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 15 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.59\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 16 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:58.\n",
            "\n",
            "  Average training loss: 0.17\n",
            "  Training epcoh took: 0:05:21\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.58\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 17 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:58.\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:05:21\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.58\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 18 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:58.\n",
            "\n",
            "  Average training loss: 0.11\n",
            "  Training epcoh took: 0:05:21\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.59\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 19 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:58.\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:05:21\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.58\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 20 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 0.08\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.58\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 21 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:58.\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:05:21\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.58\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 22 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.59\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 23 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.59\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 24 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.59\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 25 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:58.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:56.\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:05:19\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.59\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 26 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:59.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.59\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 27 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:58.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:05:20\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.59\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 28 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:58.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:57.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:05:19\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.59\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 29 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:58.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:56.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:05:19\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.59\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "======== Epoch 30 / 30 ========\n",
            "Training...\n",
            "  Batch   500  of  1,350.    Elapsed: 0:01:58.\n",
            "  Batch 1,000  of  1,350.    Elapsed: 0:03:56.\n",
            "\n",
            "  Average training loss: 0.01\n",
            "  Training epcoh took: 0:05:18\n",
            "  Model saved at /content/drive/MyDrive/hu_dataset/bert_news_model_2020_11_30_05_09_26.pth\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.59\n",
            "  Validation took: 0:00:10\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGKzE5uXMocG"
      },
      "source": [
        "#**Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJu2skuzCANS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3611d79-3d37-46c4-8b08-eb7cd309af78"
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=NUM_LABELS)\n",
        "model.cuda()\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=85, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BVbl4Zjatzn"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# **Test Set Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5KHb6RkbHdj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e71cb5d-6910-4cfb-90f8-6dc0244d3e5b"
      },
      "source": [
        "#시작 시간 설정\n",
        "t0 = time.time()\n",
        "\n",
        "# 평가모드로 변경\n",
        "model.eval()\n",
        "\n",
        "# 변수 초기화\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "# 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "for step, batch in enumerate(test_dataloader):\n",
        "    # 경과 정보 표시\n",
        "    if step % 100 == 0 and not step == 0:\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
        "\n",
        "    # 배치를 GPU에 넣음\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # 배치에서 데이터 추출\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # 그래디언트 계산 안함\n",
        "    with torch.no_grad():     \n",
        "        # Forward 수행\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "    \n",
        "    # 로스 구함\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # CPU로 데이터 이동\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "print(\"\")\n",
        "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Batch   100  of    375.    Elapsed: 0:00:07.\n",
            "  Batch   200  of    375.    Elapsed: 0:00:13.\n",
            "  Batch   300  of    375.    Elapsed: 0:00:20.\n",
            "\n",
            "Accuracy: 0.58\n",
            "Test took: 0:00:25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7SzL1IBe1Dm"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "# **NEW topic Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb4v_VfEfGQB"
      },
      "source": [
        "# 입력 데이터 변환\n",
        "def convert_input_data(sentences):\n",
        "\n",
        "    # BERT의 토크나이저로 문장을 토큰으로 분리\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "    # 입력 토큰의 최대 시퀀스 길이\n",
        "    MAX_LEN = 128\n",
        "\n",
        "    # 토큰을 숫자 인덱스로 변환\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    \n",
        "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    # 어텐션 마스크 초기화\n",
        "    attention_masks = []\n",
        "\n",
        "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # 데이터를 파이토치의 텐서로 변환\n",
        "    inputs = torch.tensor(input_ids)\n",
        "    masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return inputs, masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C12NL1Fvgv4E"
      },
      "source": [
        "# 문장 테스트\n",
        "def test_sentences(sentences):\n",
        "\n",
        "    # 평가모드로 변경\n",
        "    model.eval()\n",
        "\n",
        "    # 문장을 입력 데이터로 변환\n",
        "    inputs, masks = convert_input_data(sentences)\n",
        "\n",
        "    # 데이터를 GPU에 넣음\n",
        "    b_input_ids = inputs.to(device)\n",
        "    b_input_mask = masks.to(device)\n",
        "            \n",
        "    # 그래디언트 계산 안함\n",
        "    with torch.no_grad():     \n",
        "        # Forward 수행\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    # 로스 구함\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # CPU로 데이터 이동\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQezr0tljJlM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dfc821d-614f-4a36-8781-85a284d0a40a"
      },
      "source": [
        "logits = test_sentences(['경찰 무력 시위 강압 폭행'])\n",
        "\n",
        "print(logits)\n",
        "print(np.argmax(logits))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.5553014   1.1636798   4.331775    0.284531    0.701762    0.18182372\n",
            "  -0.81259525 -0.6040593   0.73273855 -0.07076877  0.05800076  1.6265992\n",
            "  -0.6516759  -0.79124063  1.6689017  -1.013213    1.4717002  -0.16425364\n",
            "  -0.93220574 -0.31989643  1.8041921  -0.4293834  -1.014419   -1.1526823\n",
            "  -0.63746774 -1.6706144  -0.03637895  0.14784603  0.76720166 -0.43307778\n",
            "  -0.7251667  -0.48204148 -0.2615093  -0.17751452 -0.7837909  -0.83278006\n",
            "  -1.6937932  -0.64513594 -0.4853816   3.1254256  -0.17023526 -0.5775923\n",
            "   0.06924354  0.65088516 -0.319876   -0.8639546   0.2839826  -0.2693283\n",
            "   0.29786462 -1.0309901  -0.38604105 -0.8095653   0.3269498  -0.04358091\n",
            "  -0.6004411  -0.7257758   0.9699153   0.66252625 -0.15584436  0.01081265\n",
            "  -0.44517967 -0.8007265   0.24600221 -1.0492593   0.28927797 -0.32362634\n",
            "  -0.60572743 -0.59088296 -0.97560203 -0.21741377 -0.13653731 -1.0215555\n",
            "  -0.14815433 -1.2362684  -0.47499362 -0.8426772   0.5446229   0.16645676\n",
            "  -0.9795745  -0.68315876 -0.6824562  -0.5328635  -0.98858774 -1.2208331\n",
            "  -0.6203224 ]]\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}